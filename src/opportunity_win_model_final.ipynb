{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "132b6012-adab-486c-a099-5cf3ed2aabd4",
   "metadata": {},
   "source": [
    "# Opportunity Win Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4c80642-9600-4341-a146-8f8f0ce93626",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import (classification_report, confusion_matrix,\n",
    "                             accuracy_score, precision_score, recall_score,\n",
    "                             f1_score, roc_auc_score, roc_curve)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fb8445-20c8-4c4f-8fac-a7063a11057c",
   "metadata": {},
   "source": [
    "### 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76f0ee19-be7c-4095-b68b-c775e2509837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1: Loading Data...\n",
      "Dataset loaded: (8800, 70)\n",
      "\n",
      "First 3 rows:\n",
      "   sales_agent  account engage_date  close_date  close_value  sales_price  \\\n",
      "0           20        8  2016-10-20  2017-03-01       1054.0         1096   \n",
      "1            6       38  2016-10-25  2017-03-11       4514.0         4821   \n",
      "2            6        8  2016-10-25  2017-03-07         50.0           55   \n",
      "\n",
      "   year_established  revenue  employees  office_location  has_close_date  \\\n",
      "0            2001.0   718.62     2448.0               14               1   \n",
      "1            2002.0  3178.24     4540.0               14               1   \n",
      "2            2001.0   718.62     2448.0               14               1   \n",
      "\n",
      "   close_value_log  revenue_log  employees_log  product_GTK 500  \\\n",
      "0         6.961296     6.578723       7.803435                0   \n",
      "1         8.415160     8.064397       8.420903                0   \n",
      "2         3.931826     6.578723       7.803435                0   \n",
      "\n",
      "   product_GTX BASIC  product_GTX PLUS BASIC  product_GTX PLUS PRO  \\\n",
      "0                  0                       1                     0   \n",
      "1                  0                       0                     0   \n",
      "2                  0                       0                     0   \n",
      "\n",
      "   product_GTX PRO  product_MG ADVANCED  product_MG SPECIAL  \\\n",
      "0                0                    0                   0   \n",
      "1                1                    0                   0   \n",
      "2                0                    0                   1   \n",
      "\n",
      "   manager_CARA LOSCH  manager_CELIA ROUCHE  manager_DUSTIN BRINKMANN  \\\n",
      "0                   0                     0                         1   \n",
      "1                   0                     0                         0   \n",
      "2                   0                     0                         0   \n",
      "\n",
      "   manager_MELVIN MARXEN  manager_ROCCO NEUBERT  manager_SUMMER SEWALD  \\\n",
      "0                      0                      0                      0   \n",
      "1                      1                      0                      0   \n",
      "2                      1                      0                      0   \n",
      "\n",
      "   regional_office_CENTRAL  regional_office_EAST  regional_office_WEST  \\\n",
      "0                        1                     0                     0   \n",
      "1                        1                     0                     0   \n",
      "2                        1                     0                     0   \n",
      "\n",
      "   series_GTK  series_GTX  series_MG  sector_EMPLOYMENT  sector_ENTERTAINMENT  \\\n",
      "0           0           1          0                  0                     0   \n",
      "1           0           1          0                  0                     0   \n",
      "2           0           0          1                  0                     0   \n",
      "\n",
      "   sector_FINANCE  sector_MARKETING  sector_MEDICAL  sector_RETAIL  \\\n",
      "0               0                 0               0              1   \n",
      "1               0                 0               1              0   \n",
      "2               0                 0               0              1   \n",
      "\n",
      "   sector_SERVICES  sector_SOFTWARE  sector_TECHNOLGY  \\\n",
      "0                0                0                 0   \n",
      "1                0                0                 0   \n",
      "2                0                0                 0   \n",
      "\n",
      "   sector_TELECOMMUNICATIONS  subsidiary_of_ACME CORPORATION  \\\n",
      "0                          0                               1   \n",
      "1                          0                               1   \n",
      "2                          0                               1   \n",
      "\n",
      "   subsidiary_of_BUBBA GUMP  subsidiary_of_GOLDDEX  subsidiary_of_INITY  \\\n",
      "0                         0                      0                    0   \n",
      "1                         0                      0                    0   \n",
      "2                         0                      0                    0   \n",
      "\n",
      "   subsidiary_of_MASSIVE DYNAMIC  subsidiary_of_SONRON  \\\n",
      "0                              0                     0   \n",
      "1                              0                     0   \n",
      "2                              0                     0   \n",
      "\n",
      "   subsidiary_of_WAREPHASE  deal_stage_ENGAGING  deal_stage_LOST  \\\n",
      "0                        0                    0                0   \n",
      "1                        0                    0                0   \n",
      "2                        0                    0                0   \n",
      "\n",
      "   deal_stage_PROSPECTING  deal_stage_WON  sales_price_scaled  decade_1970s  \\\n",
      "0                       0               1           -0.488043             0   \n",
      "1                       0               1            0.928795             0   \n",
      "2                       0               1           -0.883996             0   \n",
      "\n",
      "   decade_1980s  decade_1990s  decade_2000s  decade_2010s  engage_year  \\\n",
      "0             0             0             1             0       2016.0   \n",
      "1             0             0             1             0       2016.0   \n",
      "2             0             0             1             0       2016.0   \n",
      "\n",
      "   engage_month  engage_dayofweek  days_to_close  closed_within_30d  \\\n",
      "0          10.0               3.0          132.0                  0   \n",
      "1          10.0               1.0          137.0                  0   \n",
      "2          10.0               1.0          133.0                  0   \n",
      "\n",
      "   account_age  rev_per_employee  agent_closed_deals  won_deal  \\\n",
      "0         24.0          0.293554                 195         1   \n",
      "1         23.0          0.700053                 553         1   \n",
      "2         24.0          0.293554                 553         1   \n",
      "\n",
      "   account_win_rate  \n",
      "0          0.920792  \n",
      "1          0.941176  \n",
      "2          0.920792  \n",
      "\n",
      "Column names:\n",
      "['sales_agent', 'account', 'engage_date', 'close_date', 'close_value', 'sales_price', 'year_established', 'revenue', 'employees', 'office_location', 'has_close_date', 'close_value_log', 'revenue_log', 'employees_log', 'product_GTK 500', 'product_GTX BASIC', 'product_GTX PLUS BASIC', 'product_GTX PLUS PRO', 'product_GTX PRO', 'product_MG ADVANCED', 'product_MG SPECIAL', 'manager_CARA LOSCH', 'manager_CELIA ROUCHE', 'manager_DUSTIN BRINKMANN', 'manager_MELVIN MARXEN', 'manager_ROCCO NEUBERT', 'manager_SUMMER SEWALD', 'regional_office_CENTRAL', 'regional_office_EAST', 'regional_office_WEST', 'series_GTK', 'series_GTX', 'series_MG', 'sector_EMPLOYMENT', 'sector_ENTERTAINMENT', 'sector_FINANCE', 'sector_MARKETING', 'sector_MEDICAL', 'sector_RETAIL', 'sector_SERVICES', 'sector_SOFTWARE', 'sector_TECHNOLGY', 'sector_TELECOMMUNICATIONS', 'subsidiary_of_ACME CORPORATION', 'subsidiary_of_BUBBA GUMP', 'subsidiary_of_GOLDDEX', 'subsidiary_of_INITY', 'subsidiary_of_MASSIVE DYNAMIC', 'subsidiary_of_SONRON', 'subsidiary_of_WAREPHASE', 'deal_stage_ENGAGING', 'deal_stage_LOST', 'deal_stage_PROSPECTING', 'deal_stage_WON', 'sales_price_scaled', 'decade_1970s', 'decade_1980s', 'decade_1990s', 'decade_2000s', 'decade_2010s', 'engage_year', 'engage_month', 'engage_dayofweek', 'days_to_close', 'closed_within_30d', 'account_age', 'rev_per_employee', 'agent_closed_deals', 'won_deal', 'account_win_rate']\n"
     ]
    }
   ],
   "source": [
    "print(\"STEP 1: Loading Data...\")\n",
    "\n",
    "# For local file\n",
    "df = pd.read_csv(\"../data/processed/cleaned_data.csv\")\n",
    "\n",
    "print(f\"Dataset loaded: {df.shape}\")\n",
    "print(f\"\\nFirst 3 rows:\")\n",
    "print(df.head(3))\n",
    "print(\"\\nColumn names:\")\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e611403f-0a95-47d1-a25a-72800657e901",
   "metadata": {},
   "source": [
    "### 2. Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51186734-e8b3-43f7-a59b-8465959b251f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data types:\n",
      "sales_agent             int64\n",
      "account                 int64\n",
      "engage_date            object\n",
      "close_date             object\n",
      "close_value           float64\n",
      "                       ...   \n",
      "account_age           float64\n",
      "rev_per_employee      float64\n",
      "agent_closed_deals      int64\n",
      "won_deal                int64\n",
      "account_win_rate      float64\n",
      "Length: 70, dtype: object\n",
      "\n",
      "Missing values:\n",
      "engage_date          500\n",
      "close_date          2089\n",
      "close_value         2089\n",
      "close_value_log     2089\n",
      "engage_year          500\n",
      "engage_month         500\n",
      "engage_dayofweek     500\n",
      "days_to_close       2089\n",
      "dtype: int64\n",
      "\n",
      "✅ Target variable 'won_deal' found\n",
      "Win rate: 76.26%\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nData types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "print(\"\\nMissing values:\")\n",
    "missing = df.isnull().sum()\n",
    "print(missing[missing > 0])\n",
    "\n",
    "# Check if we have a target variable\n",
    "if 'won_deal' in df.columns:\n",
    "    print(\"\\n✅ Target variable 'won_deal' found\")\n",
    "    print(f\"Win rate: {df['won_deal'].mean():.2%}\")\n",
    "elif 'deal_stage' in df.columns:\n",
    "    print(\"\\n✅ Creating target from 'deal_stage'\")\n",
    "    df['won_deal'] = (df['deal_stage'] == 'WON').astype(int)\n",
    "    print(f\"Win rate: {df['won_deal'].mean():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c371a2ad-db4d-4978-aaff-3aaaea02f8e4",
   "metadata": {},
   "source": [
    "### 3. Remove Leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5dfe7204-3ac0-4e23-9700-0282b55e8ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Removed 14 leakage features:\n",
      "   - close_date\n",
      "   - close_value\n",
      "   - has_close_date\n",
      "   - close_value_log\n",
      "   - days_to_close\n",
      "   - closed_within_30d\n",
      "   - deal_stage_WON\n",
      "   - deal_stage_LOST\n",
      "   - deal_stage_ENGAGING\n",
      "   - deal_stage_PROSPECTING\n",
      "   - agent_closed_deals\n",
      "   - account_win_rate\n",
      "   - account\n",
      "   - sales_agent\n",
      "\n",
      "✅ Clean dataset shape: (8800, 56)\n"
     ]
    }
   ],
   "source": [
    "# CRITICAL: These features cause data leakage\n",
    "leakage_features = [\n",
    "    # Direct leakage - only known after outcome\n",
    "    'close_date',           # Only exists if deal closed\n",
    "    'close_value',          # Only known after close\n",
    "    'has_close_date',       # Reveals if deal closed\n",
    "    'close_value_log',      # Derived from close_value\n",
    "    'days_to_close',        # Can only calculate after close\n",
    "    'closed_within_30d',    # Only known after close\n",
    "    \n",
    "    # Target leakage - these ARE the answer\n",
    "    'deal_stage_WON',       # This is what we're predicting!\n",
    "    'deal_stage_LOST',      # This is what we're predicting!\n",
    "    'deal_stage_ENGAGING',  # Current stage reveals outcome\n",
    "    'deal_stage_PROSPECTING',\n",
    "    \n",
    "    # Aggregation leakage - includes future information\n",
    "    'agent_closed_deals',   # May include current deal\n",
    "    'account_win_rate',     # CRITICAL: Includes future deals and depenpency on if a deal is closed\n",
    "    \n",
    "    # Identifiers (not useful for prediction)\n",
    "    'opportunity_id',\n",
    "    'deal_stage',\n",
    "    'account',              # Too many unique values, use features derived from it\n",
    "    'sales_agent',          # Will encode this properly\n",
    "]\n",
    "\n",
    "# Remove leakage columns that exist\n",
    "leakage_removed = [col for col in leakage_features if col in df.columns]\n",
    "df_clean = df.drop(columns=leakage_removed, errors='ignore')\n",
    "\n",
    "print(f\"❌ Removed {len(leakage_removed)} leakage features:\")\n",
    "for col in leakage_removed:\n",
    "    print(f\"   - {col}\")\n",
    "\n",
    "print(f\"\\n✅ Clean dataset shape: {df_clean.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c81c65-bd6f-470a-a3df-f57c45eace28",
   "metadata": {},
   "source": [
    "### 4. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17305297-ffb5-4c2a-acfa-b6c0cb2685eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created temporal features from engage_date\n",
      "✅ Created company_age feature\n",
      "✅ Created revenue_per_employee feature\n",
      "✅ Created price_to_revenue_ratio feature\n",
      "✅ Created log-transformed features\n"
     ]
    }
   ],
   "source": [
    "# Convert dates\n",
    "if 'engage_date' in df_clean.columns:\n",
    "    df_clean['engage_date'] = pd.to_datetime(df_clean['engage_date'], errors='coerce')\n",
    "    \n",
    "    # Extract date features\n",
    "    df_clean['engage_year'] = df_clean['engage_date'].dt.year\n",
    "    df_clean['engage_month'] = df_clean['engage_date'].dt.month\n",
    "    df_clean['engage_day_of_week'] = df_clean['engage_date'].dt.dayofweek\n",
    "    df_clean['engage_quarter'] = df_clean['engage_date'].dt.quarter\n",
    "    df_clean['is_month_end'] = (df_clean['engage_date'].dt.day > 25).astype(int)\n",
    "    df_clean['is_quarter_end'] = (df_clean['engage_month'] % 3 == 0).astype(int)\n",
    "    \n",
    "    print(\"✅ Created temporal features from engage_date\")\n",
    "\n",
    "# Company age\n",
    "if 'year_established' in df_clean.columns:\n",
    "    df_clean['company_age'] = 2017 - df_clean['year_established']\n",
    "    print(\"✅ Created company_age feature\")\n",
    "\n",
    "# Revenue per employee (efficiency metric)\n",
    "if 'revenue' in df_clean.columns and 'employees' in df_clean.columns:\n",
    "    df_clean['revenue_per_employee'] = df_clean['revenue'] / (df_clean['employees'] + 1)\n",
    "    print(\"✅ Created revenue_per_employee feature\")\n",
    "\n",
    "# Price to revenue ratio (deal size relative to company)\n",
    "if 'sales_price' in df_clean.columns and 'revenue' in df_clean.columns:\n",
    "    df_clean['price_to_revenue_ratio'] = df_clean['sales_price'] / (df_clean['revenue'] + 1)\n",
    "    print(\"✅ Created price_to_revenue_ratio feature\")\n",
    "\n",
    "# Log transforms for skewed features\n",
    "if 'revenue' in df_clean.columns:\n",
    "    df_clean['revenue_log'] = np.log1p(df_clean['revenue'])\n",
    "    \n",
    "if 'employees' in df_clean.columns:\n",
    "    df_clean['employees_log'] = np.log1p(df_clean['employees'])\n",
    "    \n",
    "if 'sales_price' in df_clean.columns:\n",
    "    df_clean['sales_price_log'] = np.log1p(df_clean['sales_price'])\n",
    "    \n",
    "print(\"✅ Created log-transformed features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c34338b-4bb7-49e7-a447-b7f7e89b4358",
   "metadata": {},
   "source": [
    "### 5. Feature Prep and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "724d645b-e682-481b-9983-164be70851f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: (8800, 61)\n",
      "Target shape: (8800,)\n",
      "\n",
      "Using 61 features:\n",
      "   1. sales_price\n",
      "   2. year_established\n",
      "   3. revenue\n",
      "   4. employees\n",
      "   5. revenue_log\n",
      "   6. employees_log\n",
      "   7. product_GTK 500\n",
      "   8. product_GTX BASIC\n",
      "   9. product_GTX PLUS BASIC\n",
      "   10. product_GTX PLUS PRO\n",
      "   11. product_GTX PRO\n",
      "   12. product_MG ADVANCED\n",
      "   13. product_MG SPECIAL\n",
      "   14. manager_CARA LOSCH\n",
      "   15. manager_CELIA ROUCHE\n",
      "   16. manager_DUSTIN BRINKMANN\n",
      "   17. manager_MELVIN MARXEN\n",
      "   18. manager_ROCCO NEUBERT\n",
      "   19. manager_SUMMER SEWALD\n",
      "   20. regional_office_CENTRAL\n",
      "   ... and 41 more\n"
     ]
    }
   ],
   "source": [
    "# Define target\n",
    "target = 'won_deal'\n",
    "y = df_clean[target].astype(int)\n",
    "\n",
    "# Remove non-feature columns\n",
    "cols_to_drop = [\n",
    "    target,\n",
    "    'engage_date',  # Already extracted features\n",
    "    'office_location',  # Keep if you want, but high cardinality\n",
    "]\n",
    "\n",
    "X = df_clean.drop(columns=[col for col in cols_to_drop if col in df_clean.columns])\n",
    "\n",
    "# Select only numeric features (encoded categorical should already be numeric)\n",
    "numeric_features = X.select_dtypes(include=['number']).columns.tolist()\n",
    "X = X[numeric_features]\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"\\nUsing {len(numeric_features)} features:\")\n",
    "for i, col in enumerate(numeric_features[:20], 1):  # Show first 20\n",
    "    print(f\"   {i}. {col}\")\n",
    "if len(numeric_features) > 20:\n",
    "    print(f\"   ... and {len(numeric_features) - 20} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9590c02-6bf1-43ff-9a65-81fcfe437ab1",
   "metadata": {},
   "source": [
    "### 6. Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "867ec6b1-7ab8-40ef-b1f9-00abcd3b5511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values before imputation:\n",
      "engage_year           500\n",
      "engage_month          500\n",
      "engage_dayofweek      500\n",
      "engage_day_of_week    500\n",
      "engage_quarter        500\n",
      "dtype: int64\n",
      "\n",
      "Imputed missing values using median strategy\n",
      "Missing values after: 0\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nMissing values before imputation:\")\n",
    "missing_before = X.isnull().sum()\n",
    "print(missing_before[missing_before > 0])\n",
    "\n",
    "# Use median imputation for numeric features\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "X = pd.DataFrame(X_imputed, columns=X.columns, index=X.index)\n",
    "\n",
    "print(f\"\\nImputed missing values using median strategy\")\n",
    "print(f\"Missing values after: {X.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d539fa8-54fd-46ce-9cc9-511a57660de9",
   "metadata": {},
   "source": [
    "### 7. Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "93c5807d-7416-4327-8a63-b2a8734574b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 7040 samples (80.0%)\n",
      "Test set: 1760 samples (20.0%)\n",
      "\n",
      "Training win rate: 76.26%\n",
      "Test win rate: 76.25%\n"
     ]
    }
   ],
   "source": [
    "# IMPORTANT: Split BEFORE any scaling to prevent train-test contamination\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(X_train)} samples ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"Test set: {len(X_test)} samples ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "print(f\"\\nTraining win rate: {y_train.mean():.2%}\")\n",
    "print(f\"Test win rate: {y_test.mean():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4bd472-0ca6-4f9c-8d3c-0e2d0c61e3ca",
   "metadata": {},
   "source": [
    "### 8. Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "90730ca2-dc5e-4bd3-905b-4675cade343e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features scaled using StandardScaler\n",
      "Scaler fit only on training data (no test contamination)\n"
     ]
    }
   ],
   "source": [
    "# Scale features - fit only on training data!\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)  # Use training statistics\n",
    "\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X.columns, index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X.columns, index=X_test.index)\n",
    "\n",
    "print(\"Features scaled using StandardScaler\")\n",
    "print(\"Scaler fit only on training data (no test contamination)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e33943-07c6-4cc5-a574-3fec68b5a2b8",
   "metadata": {},
   "source": [
    "### 9. Class Imablance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "81f8d3b4-d371-48f4-ac1f-8b8e90274cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class distribution:\n",
      "won_deal\n",
      "1    5369\n",
      "0    1671\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Win rate: 76.26%\n",
      "\n",
      "Class weights: {0: np.float64(2.106523040095751), 1: np.float64(0.6556155708698082)}\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nClass distribution:\")\n",
    "print(y_train.value_counts())\n",
    "print(f\"\\nWin rate: {y_train.mean():.2%}\")\n",
    "\n",
    "# Calculate class weights\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "class_weights = compute_class_weight(\n",
    "    'balanced', \n",
    "    classes=np.unique(y_train), \n",
    "    y=y_train\n",
    ")\n",
    "class_weight_dict = {0: class_weights[0], 1: class_weights[1]}\n",
    "print(f\"\\nClass weights: {class_weight_dict}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a1f2a4-b6c3-4168-8618-b47a65bedeeb",
   "metadata": {},
   "source": [
    "### 10. Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4f01392f-f662-40b4-b8cc-63df2c7854f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Logistic Regression...\n",
      "  Accuracy:  0.8284\n",
      "  Precision: 0.9241\n",
      "  Recall:    0.8443\n",
      "  F1 Score:  0.8824\n",
      "  ROC AUC:   0.8613\n",
      "\n",
      "Training Random Forest...\n",
      "  Accuracy:  0.9449\n",
      "  Precision: 0.9356\n",
      "  Recall:    0.9963\n",
      "  F1 Score:  0.9650\n",
      "  ROC AUC:   0.9664\n",
      "\n",
      "Training Gradient Boosting...\n",
      "  Accuracy:  0.9585\n",
      "  Precision: 0.9510\n",
      "  Recall:    0.9970\n",
      "  F1 Score:  0.9734\n",
      "  ROC AUC:   0.9714\n",
      "\n",
      "Training XGBoost...\n",
      "  Accuracy:  0.9466\n",
      "  Precision: 0.9535\n",
      "  Recall:    0.9776\n",
      "  F1 Score:  0.9654\n",
      "  ROC AUC:   0.9625\n"
     ]
    }
   ],
   "source": [
    "# Define models with class balancing\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(\n",
    "        max_iter=2000, \n",
    "        random_state=42,\n",
    "        class_weight='balanced',\n",
    "        C=0.5\n",
    "    ),\n",
    "    'Random Forest': RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=10,\n",
    "        min_samples_split=5,\n",
    "        random_state=42,\n",
    "        class_weight='balanced',\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=5,\n",
    "        learning_rate=0.05,\n",
    "        random_state=42\n",
    "    ),\n",
    "    'XGBoost': XGBClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=5,\n",
    "        learning_rate=0.05,\n",
    "        scale_pos_weight=len(y_train[y_train==0])/len(y_train[y_train==1]),\n",
    "        random_state=42,\n",
    "        eval_metric='logloss'\n",
    "    )\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    # Train\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    results[name] = {\n",
    "        'model': model,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'roc_auc': roc_auc,\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_proba': y_pred_proba\n",
    "    }\n",
    "    \n",
    "    print(f\"  Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall:    {recall:.4f}\")\n",
    "    print(f\"  F1 Score:  {f1:.4f}\")\n",
    "    print(f\"  ROC AUC:   {roc_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a1538a-9db0-4644-b545-4d66e17de32b",
   "metadata": {},
   "source": [
    "### 11. Model Comparison and Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7be5252f-1912-4147-9422-9fb4065d69c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "              Model  Accuracy  Precision   Recall  F1 Score  ROC AUC\n",
      "Logistic Regression  0.828409   0.924144 0.844262  0.882399 0.861299\n",
      "      Random Forest  0.944886   0.935619 0.996274  0.964995 0.966416\n",
      "  Gradient Boosting  0.958523   0.950959 0.997019  0.973445 0.971439\n",
      "            XGBoost  0.946591   0.953488 0.977645  0.965416 0.962549\n",
      "\n",
      "Best model: Gradient Boosting\n",
      "   F1 Score: 0.9734\n",
      "   ROC AUC: 0.9714\n",
      "\n",
      "======================================================================\n",
      "Detailed Evaluation - Gradient Boosting\n",
      "======================================================================\n",
      "\n",
      " Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Lost      0.989     0.835     0.905       418\n",
      "         Won      0.951     0.997     0.973      1342\n",
      "\n",
      "    accuracy                          0.959      1760\n",
      "   macro avg      0.970     0.916     0.939      1760\n",
      "weighted avg      0.960     0.959     0.957      1760\n",
      "\n",
      "\n",
      " Confusion Matrix:\n",
      "[[ 349   69]\n",
      " [   4 1338]]\n",
      "\n",
      "Interpretation:\n",
      "  True Negatives (Correctly predicted Lost):  349\n",
      "  False Positives (Predicted Won, Actually Lost): 69\n",
      "  False Negatives (Predicted Lost, Actually Won): 4\n",
      "  True Positives (Correctly predicted Won):   1338\n",
      "  Total actual wins: 1342\n",
      "  Wins we caught: 1338 (99.7%)\n",
      "  Wins we missed: 4 (0.3% - opportunity cost)\n",
      "  False alarms: 69 (16.5% of losses)\n"
     ]
    }
   ],
   "source": [
    "comparison_df = pd.DataFrame({\n",
    "    'Model': list(results.keys()),\n",
    "    'Accuracy': [results[m]['accuracy'] for m in results.keys()],\n",
    "    'Precision': [results[m]['precision'] for m in results.keys()],\n",
    "    'Recall': [results[m]['recall'] for m in results.keys()],\n",
    "    'F1 Score': [results[m]['f1'] for m in results.keys()],\n",
    "    'ROC AUC': [results[m]['roc_auc'] for m in results.keys()]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + comparison_df.to_string(index=False))\n",
    "\n",
    "# Find best model by F1 score (balance of precision and recall)\n",
    "best_model_name = comparison_df.loc[comparison_df['F1 Score'].idxmax(), 'Model']\n",
    "best_model = results[best_model_name]['model']\n",
    "\n",
    "print(f\"\\nBest model: {best_model_name}\")\n",
    "print(f\"   F1 Score: {results[best_model_name]['f1']:.4f}\")\n",
    "print(f\"   ROC AUC: {results[best_model_name]['roc_auc']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"Detailed Evaluation - {best_model_name}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "y_pred_best = results[best_model_name]['y_pred']\n",
    "y_pred_proba_best = results[best_model_name]['y_pred_proba']\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\n Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_best, \n",
    "                          target_names=['Lost', 'Won'],\n",
    "                          digits=3))\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\n Confusion Matrix:\")\n",
    "cm = confusion_matrix(y_test, y_pred_best)\n",
    "print(cm)\n",
    "print(\"\\nInterpretation:\")\n",
    "print(f\"  True Negatives (Correctly predicted Lost):  {cm[0,0]}\")\n",
    "print(f\"  False Positives (Predicted Won, Actually Lost): {cm[0,1]}\")\n",
    "print(f\"  False Negatives (Predicted Lost, Actually Won): {cm[1,0]}\")\n",
    "print(f\"  True Positives (Correctly predicted Won):   {cm[1,1]}\")\n",
    "\n",
    "# Calculate metrics\n",
    "total_won = cm[1,0] + cm[1,1]\n",
    "total_lost = cm[0,0] + cm[0,1]\n",
    "print(f\"  Total actual wins: {total_won}\")\n",
    "print(f\"  Wins we caught: {cm[1,1]} ({cm[1,1]/total_won*100:.1f}%)\")\n",
    "print(f\"  Wins we missed: {cm[1,0]} ({cm[1,0]/total_won*100:.1f}% - opportunity cost)\")\n",
    "print(f\"  False alarms: {cm[0,1]} ({cm[0,1]/total_lost*100:.1f}% of losses)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b843de1e-9780-43a3-b9df-102fba43e79d",
   "metadata": {},
   "source": [
    "### 12. Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3b01b2de-d5b0-40ff-b724-ee7279e8cd2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression:\n",
      "  Mean F1: 0.8827 (+/- 0.0167)\n",
      "  Individual folds: ['0.879', '0.873', '0.878', '0.896', '0.888']\n",
      "\n",
      "Random Forest:\n",
      "  Mean F1: 0.9621 (+/- 0.0048)\n",
      "  Individual folds: ['0.958', '0.961', '0.964', '0.965', '0.961']\n",
      "\n",
      "Gradient Boosting:\n",
      "  Mean F1: 0.9666 (+/- 0.0027)\n",
      "  Individual folds: ['0.965', '0.967', '0.967', '0.969', '0.966']\n",
      "\n",
      "XGBoost:\n",
      "  Mean F1: 0.9550 (+/- 0.0062)\n",
      "  Individual folds: ['0.952', '0.956', '0.958', '0.950', '0.958']\n"
     ]
    }
   ],
   "source": [
    "for name, model in models.items():\n",
    "    cv_scores = cross_val_score(model, X_train_scaled, y_train, \n",
    "                                cv=5, scoring='f1', n_jobs=-1)\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Mean F1: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "    print(f\"  Individual folds: {[f'{s:.3f}' for s in cv_scores]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f1d168-1933-41c3-b266-bebb6962200f",
   "metadata": {},
   "source": [
    "### 13. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0475c2bd-ce9b-401c-b195-14f4ec7ab5e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved as: best_model_gradient_boosting.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Save best model\n",
    "model_filename = f'best_model_{best_model_name.replace(\" \", \"_\").lower()}.pkl'\n",
    "with open(model_filename, 'wb') as f:\n",
    "    pickle.dump(best_model, f)\n",
    "print(f\"Best model saved as: {model_filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
