{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lead Scoring Model\n",
    "A lead scoring model isn’t meant to predict the final outcome of a deal — that’s the job of an opportunity win model. Instead, its purpose is to assess whether a new prospect is worth pursuing at all. That’s why any lead that progresses beyond “Prospecting” — whether it ends in “Engaging,” “Won,” or even “Lost” — is considered qualified. Moving forward in the pipeline shows that the lead was strong enough to warrant sales effort, even if it didn’t close. By defining the label this way, the model learns to distinguish promising leads from those that are unlikely to ever engage, helping sales teams prioritize their time and resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import (roc_auc_score, average_precision_score, precision_recall_curve,\n",
    "                             confusion_matrix, classification_report)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from scipy.stats import randint, uniform\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset loading and label setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(\"../data/processed/cleaned_data.csv\")\n",
    "\n",
    "# Creating target variable\n",
    "# A lead will be qualified if it moves past \"Prospecting\" stage\n",
    "# So this includes Engaging, Won, or Lost - all show the lead warranted sales attention\n",
    "df[\"qualified_lead\"] = (\n",
    "    (df.get(\"deal_stage_ENGAGING\", 0) == 1) |\n",
    "    (df.get(\"deal_stage_WON\", 0) == 1) |\n",
    "    (df.get(\"deal_stage_LOST\", 0) == 1)\n",
    ").astype(int)\n",
    "\n",
    "TARGET = \"qualified_lead\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 7040 samples\n",
      "Test set: 1760 samples\n",
      "Class distribution: {1: 0.9431818181818182, 0: 0.056818181818181816}\n"
     ]
    }
   ],
   "source": [
    "# Remove any columns that would leak the answer\n",
    "temporal_cols = ['engage_date', 'close_date', 'engage_year', 'engage_month', \n",
    "                 'engage_dayofweek', 'days_to_close', 'closed_within_30d']\n",
    "outcome_cols = ['deal_stage_PROSPECTING', 'deal_stage_ENGAGING', \n",
    "                'deal_stage_WON', 'deal_stage_LOST', 'won_deal', \n",
    "                'has_close_date', 'close_value', 'close_value_log']\n",
    "remove_cols = temporal_cols + outcome_cols\n",
    "\n",
    "# Keeping transformed features, droping raw versions\n",
    "raw_features = ['revenue', 'employees', 'sales_price']\n",
    "\n",
    "# Preparing training and test sets\n",
    "feature_cols = [c for c in df.columns if c not in remove_cols + raw_features + [TARGET]]\n",
    "X = df[feature_cols]\n",
    "y = df[TARGET]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"Class distribution: {y_train.value_counts(normalize=True).to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "BASELINE MODEL COMPARISON\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression\n",
      "  ROC-AUC: 0.9471\n",
      "  PR-AUC: 0.9966\n",
      "  Confusion Matrix:\n",
      "[[  88   12]\n",
      " [ 267 1393]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.248     0.880     0.387       100\n",
      "           1      0.991     0.839     0.909      1660\n",
      "\n",
      "    accuracy                          0.841      1760\n",
      "   macro avg      0.620     0.860     0.648      1760\n",
      "weighted avg      0.949     0.841     0.879      1760\n",
      "\n",
      "\n",
      "Random Forest\n",
      "  ROC-AUC: 0.9567\n",
      "  PR-AUC: 0.9972\n",
      "  Confusion Matrix:\n",
      "[[  84   16]\n",
      " [  91 1569]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.480     0.840     0.611       100\n",
      "           1      0.990     0.945     0.967      1660\n",
      "\n",
      "    accuracy                          0.939      1760\n",
      "   macro avg      0.735     0.893     0.789      1760\n",
      "weighted avg      0.961     0.939     0.947      1760\n",
      "\n",
      "\n",
      "XGBoost\n",
      "  ROC-AUC: 0.9495\n",
      "  PR-AUC: 0.9967\n",
      "  Confusion Matrix:\n",
      "[[  85   15]\n",
      " [ 117 1543]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.421     0.850     0.563       100\n",
      "           1      0.990     0.930     0.959      1660\n",
      "\n",
      "    accuracy                          0.925      1760\n",
      "   macro avg      0.706     0.890     0.761      1760\n",
      "weighted avg      0.958     0.925     0.936      1760\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Baseline model comparison\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BASELINE MODEL COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def evaluate_classifier(name, model, X_tr, y_tr, X_te, y_te):\n",
    "    \"\"\"train and evaluate a model\"\"\"\n",
    "    model.fit(X_tr, y_tr)\n",
    "    probs = model.predict_proba(X_te)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    roc = roc_auc_score(y_te, probs)\n",
    "    pr_auc = average_precision_score(y_te, probs)\n",
    "    \n",
    "    # Default 0.5 threshold predictions\n",
    "    preds = (probs >= 0.5).astype(int)\n",
    "    cm = confusion_matrix(y_te, preds)\n",
    "    \n",
    "    print(f\"\\n{name}\")\n",
    "    print(f\"  ROC-AUC: {roc:.4f}\")\n",
    "    print(f\"  PR-AUC: {pr_auc:.4f}\")\n",
    "    print(f\"  Confusion Matrix:\\n{cm}\")\n",
    "    print(classification_report(y_te, preds, digits=3))\n",
    "    \n",
    "    return probs, {\"roc_auc\": roc, \"pr_auc\": pr_auc}\n",
    "\n",
    "# Model 1: Logistic Regression (Simple baseline)\n",
    "lr_model = LogisticRegression(max_iter=1000, class_weight=\"balanced\", random_state=42)\n",
    "lr_probs, lr_metrics = evaluate_classifier(\"Logistic Regression\", lr_model, \n",
    "                                           X_train, y_train, X_test, y_test)\n",
    "\n",
    "# Model 2: Random Forest (Tree-based ensemble)\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=400, max_depth=None, min_samples_leaf=2,\n",
    "    class_weight=\"balanced_subsample\", random_state=42, n_jobs=-1\n",
    ")\n",
    "rf_probs, rf_metrics = evaluate_classifier(\"Random Forest\", rf_model,\n",
    "                                           X_train, y_train, X_test, y_test)\n",
    "\n",
    "# Model 3: XGBoost (Gradient boosting)\n",
    "scale_weight = (y_train.value_counts()[0] / y_train.value_counts()[1])\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=600, learning_rate=0.05, max_depth=6,\n",
    "    subsample=0.8, colsample_bytree=0.8, reg_lambda=1.0,\n",
    "    scale_pos_weight=scale_weight, tree_method=\"hist\",\n",
    "    eval_metric=\"logloss\", random_state=42\n",
    ")\n",
    "xgb_probs, xgb_metrics = evaluate_classifier(\"XGBoost\", xgb_model,\n",
    "                                             X_train, y_train, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter tuning using Random Search + Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "HYPERPARAMETER TUNING (Random Forest)\n",
      "============================================================\n",
      "\n",
      "Phase 1: Random Search...\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV PR-AUC: 0.9972\n",
      "Best parameters: {'bootstrap': False, 'max_depth': 11, 'max_features': 0.16357131438718206, 'min_samples_leaf': 2, 'min_samples_split': 21, 'n_estimators': 341}\n",
      "\n",
      "Phase 2: Grid Search Refinement...\n",
      "Fitting 5 folds for each of 162 candidates, totalling 810 fits\n",
      "Refined CV PR-AUC: 0.9972\n",
      "Final parameters: {'bootstrap': False, 'max_depth': 9, 'max_features': 0.16357131438718206, 'min_samples_leaf': 2, 'min_samples_split': 24, 'n_estimators': 341}\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning with Random Forest\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"HYPERPARAMETER TUNING (Random Forest)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Cross-validation strategy\n",
    "cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Broad random search\n",
    "param_distributions = {\n",
    "    \"n_estimators\": randint(250, 1000),\n",
    "    \"max_depth\": randint(3, 35),\n",
    "    \"min_samples_split\": randint(2, 25),\n",
    "    \"min_samples_leaf\": randint(1, 12),\n",
    "    \"max_features\": uniform(0.15, 0.85),\n",
    "    \"bootstrap\": [True, False],\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    RandomForestClassifier(random_state=42, class_weight=\"balanced_subsample\", n_jobs=-1),\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=50,\n",
    "    scoring=\"average_precision\",\n",
    "    cv=cv_strategy,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nPhase 1: Random Search...\")\n",
    "random_search.fit(X_train, y_train)\n",
    "print(f\"Best CV PR-AUC: {random_search.best_score_:.4f}\")\n",
    "print(f\"Best parameters: {random_search.best_params_}\")\n",
    "\n",
    "# Fine-tuning with grid search\n",
    "best = random_search.best_params_\n",
    "param_grid = {\n",
    "    \"n_estimators\": [best[\"n_estimators\"], best[\"n_estimators\"] + 150],\n",
    "    \"max_depth\": [max(3, best[\"max_depth\"]-2), best[\"max_depth\"], best[\"max_depth\"]+2],\n",
    "    \"min_samples_split\": [max(2, best[\"min_samples_split\"]-3), \n",
    "                          best[\"min_samples_split\"], \n",
    "                          best[\"min_samples_split\"]+3],\n",
    "    \"min_samples_leaf\": [max(1, best[\"min_samples_leaf\"]-1), \n",
    "                         best[\"min_samples_leaf\"], \n",
    "                         best[\"min_samples_leaf\"]+1],\n",
    "    \"max_features\": [max(0.1, best[\"max_features\"]-0.15), \n",
    "                     best[\"max_features\"], \n",
    "                     min(1.0, best[\"max_features\"]+0.15)],\n",
    "    \"bootstrap\": [best[\"bootstrap\"]],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    RandomForestClassifier(random_state=42, class_weight=\"balanced_subsample\", n_jobs=-1),\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"average_precision\",\n",
    "    cv=cv_strategy,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nPhase 2: Grid Search Refinement...\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(f\"Refined CV PR-AUC: {grid_search.best_score_:.4f}\")\n",
    "print(f\"Final parameters: {grid_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FINAL MODEL PERFORMANCE\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Final model evaluation\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL MODEL PERFORMANCE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "final_model = grid_search.best_estimator_\n",
    "final_probs = final_model.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threshold Selection + Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimal threshold: 0.1932\n",
      "  Precision: 0.989\n",
      "  Recall: 0.962\n",
      "  F1-Score: 0.975\n",
      "\n",
      "Final Test Performance:\n",
      "  ROC-AUC: 0.9520\n",
      "  PR-AUC: 0.9968\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  82   18]\n",
      " [  63 1597]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.566     0.820     0.669       100\n",
      "           1      0.989     0.962     0.975      1660\n",
      "\n",
      "    accuracy                          0.954      1760\n",
      "   macro avg      0.777     0.891     0.822      1760\n",
      "weighted avg      0.965     0.954     0.958      1760\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Finding optimal threshold\n",
    "def find_optimal_threshold(y_true, probabilities):\n",
    "    \"\"\"find threshold that maximizes F1\"\"\"\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, probabilities)\n",
    "    \n",
    "    # Calculate F1 scores\n",
    "    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
    "    \n",
    "    # Find best threshold\n",
    "    best_idx = np.nanargmax(f1_scores)\n",
    "    optimal_threshold = thresholds[best_idx] if best_idx < len(thresholds) else 0.5\n",
    "    \n",
    "    return optimal_threshold, precision[best_idx], recall[best_idx], f1_scores[best_idx]\n",
    "\n",
    "# Find optimal threshold\n",
    "opt_thresh, opt_prec, opt_rec, opt_f1 = find_optimal_threshold(y_test, final_probs)\n",
    "print(f\"\\nOptimal threshold: {opt_thresh:.4f}\")\n",
    "print(f\"  Precision: {opt_prec:.3f}\")\n",
    "print(f\"  Recall: {opt_rec:.3f}\")\n",
    "print(f\"  F1-Score: {opt_f1:.3f}\")\n",
    "\n",
    "# Evaluate at optimal threshold\n",
    "final_preds = (final_probs >= opt_thresh).astype(int)\n",
    "final_cm = confusion_matrix(y_test, final_preds)\n",
    "\n",
    "print(f\"\\nFinal Test Performance:\")\n",
    "print(f\"  ROC-AUC: {roc_auc_score(y_test, final_probs):.4f}\")\n",
    "print(f\"  PR-AUC: {average_precision_score(y_test, final_probs):.4f}\")\n",
    "print(f\"\\nConfusion Matrix:\\n{final_cm}\")\n",
    "print(classification_report(y_test, final_preds, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model saved to lead_scoring_model.pkl\n"
     ]
    }
   ],
   "source": [
    "# Export model\n",
    "import pickle\n",
    "\n",
    "model_package = {\n",
    "    \"classifier\": final_model,\n",
    "    \"optimal_threshold\": opt_thresh,\n",
    "    \"hyperparameters\": grid_search.best_params_,\n",
    "    \"cv_score\": grid_search.best_score_,\n",
    "    \"test_metrics\": {\n",
    "        \"roc_auc\": roc_auc_score(y_test, final_probs),\n",
    "        \"pr_auc\": average_precision_score(y_test, final_probs),\n",
    "        \"f1\": opt_f1\n",
    "    },\n",
    "    \"model_version\": \"1.0\",\n",
    "    \"notes\": \"Optimized Random Forest for lead qualification with custom threshold\"\n",
    "}\n",
    "\n",
    "with open(\"../models/lead_scoring_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model_package, f)\n",
    "\n",
    "print(\"\\nModel saved to lead_scoring_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
