{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lead Scoring Model\n",
    "A lead scoring model isn’t meant to predict the final outcome of a deal — that’s the job of an opportunity win model. Instead, its purpose is to assess whether a new prospect is worth pursuing at all. That’s why any lead that progresses beyond “Prospecting” — whether it ends in “Engaging,” “Won,” or even “Lost” — is considered qualified. Moving forward in the pipeline shows that the lead was strong enough to warrant sales effort, even if it didn’t close. By defining the label this way, the model learns to distinguish promising leads from those that are unlikely to ever engage, helping sales teams prioritize their time and resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import (roc_auc_score, average_precision_score, precision_recall_curve,\n",
    "                             confusion_matrix, classification_report)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from scipy.stats import randint, uniform\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset loading and label setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(\"../data/processed/cleaned_data.csv\")\n",
    "\n",
    "# Creating target variable\n",
    "# A lead will be qualified if it moves past \"Prospecting\" stage\n",
    "# So this includes Engaging, Won, or Lost - all show the lead warranted sales attention\n",
    "df[\"qualified_lead\"] = (\n",
    "    (df.get(\"deal_stage_ENGAGING\", 0) == 1) |\n",
    "    (df.get(\"deal_stage_WON\", 0) == 1) |\n",
    "    (df.get(\"deal_stage_LOST\", 0) == 1)\n",
    ").astype(int)\n",
    "\n",
    "TARGET = \"qualified_lead\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any columns that would leak the answer\n",
    "temporal_cols = ['engage_date', 'close_date', 'engage_year', 'engage_month', \n",
    "                 'engage_dayofweek', 'days_to_close', 'closed_within_30d']\n",
    "outcome_cols = ['deal_stage_PROSPECTING', 'deal_stage_ENGAGING', \n",
    "                'deal_stage_WON', 'deal_stage_LOST', 'won_deal', \n",
    "                'has_close_date', 'close_value', 'close_value_log']\n",
    "remove_cols = temporal_cols + outcome_cols\n",
    "\n",
    "# Keeping transformed features, droping raw versions\n",
    "raw_features = ['revenue', 'employees', 'sales_price']\n",
    "\n",
    "# Preparing training and test sets\n",
    "feature_cols = [c for c in df.columns if c not in remove_cols + raw_features + [TARGET]]\n",
    "X = df[feature_cols]\n",
    "y = df[TARGET]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"Class distribution: {y_train.value_counts(normalize=True).to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline model comparison\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BASELINE MODEL COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def evaluate_classifier(name, model, X_tr, y_tr, X_te, y_te):\n",
    "    \"\"\"train and evaluate a model\"\"\"\n",
    "    model.fit(X_tr, y_tr)\n",
    "    probs = model.predict_proba(X_te)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    roc = roc_auc_score(y_te, probs)\n",
    "    pr_auc = average_precision_score(y_te, probs)\n",
    "    \n",
    "    # Default 0.5 threshold predictions\n",
    "    preds = (probs >= 0.5).astype(int)\n",
    "    cm = confusion_matrix(y_te, preds)\n",
    "    \n",
    "    print(f\"\\n{name}\")\n",
    "    print(f\"  ROC-AUC: {roc:.4f}\")\n",
    "    print(f\"  PR-AUC: {pr_auc:.4f}\")\n",
    "    print(f\"  Confusion Matrix:\\n{cm}\")\n",
    "    print(classification_report(y_te, preds, digits=3))\n",
    "    \n",
    "    return probs, {\"roc_auc\": roc, \"pr_auc\": pr_auc}\n",
    "\n",
    "# Model 1: Logistic Regression (Simple baseline)\n",
    "lr_model = LogisticRegression(max_iter=1000, class_weight=\"balanced\", random_state=42)\n",
    "lr_probs, lr_metrics = evaluate_classifier(\"Logistic Regression\", lr_model, \n",
    "                                           X_train, y_train, X_test, y_test)\n",
    "\n",
    "# Model 2: Random Forest (Tree-based ensemble)\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=400, max_depth=None, min_samples_leaf=2,\n",
    "    class_weight=\"balanced_subsample\", random_state=42, n_jobs=-1\n",
    ")\n",
    "rf_probs, rf_metrics = evaluate_classifier(\"Random Forest\", rf_model,\n",
    "                                           X_train, y_train, X_test, y_test)\n",
    "\n",
    "# Model 3: XGBoost (Gradient boosting)\n",
    "scale_weight = (y_train.value_counts()[0] / y_train.value_counts()[1])\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=600, learning_rate=0.05, max_depth=6,\n",
    "    subsample=0.8, colsample_bytree=0.8, reg_lambda=1.0,\n",
    "    scale_pos_weight=scale_weight, tree_method=\"hist\",\n",
    "    eval_metric=\"logloss\", random_state=42\n",
    ")\n",
    "xgb_probs, xgb_metrics = evaluate_classifier(\"XGBoost\", xgb_model,\n",
    "                                             X_train, y_train, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter tuning using Random Search + Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning with Random Forest\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"HYPERPARAMETER TUNING (Random Forest)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Cross-validation strategy\n",
    "cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Broad random search\n",
    "param_distributions = {\n",
    "    \"n_estimators\": randint(250, 1000),\n",
    "    \"max_depth\": randint(3, 35),\n",
    "    \"min_samples_split\": randint(2, 25),\n",
    "    \"min_samples_leaf\": randint(1, 12),\n",
    "    \"max_features\": uniform(0.15, 0.85),\n",
    "    \"bootstrap\": [True, False],\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    RandomForestClassifier(random_state=42, class_weight=\"balanced_subsample\", n_jobs=-1),\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=50,\n",
    "    scoring=\"average_precision\",\n",
    "    cv=cv_strategy,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nPhase 1: Random Search...\")\n",
    "random_search.fit(X_train, y_train)\n",
    "print(f\"Best CV PR-AUC: {random_search.best_score_:.4f}\")\n",
    "print(f\"Best parameters: {random_search.best_params_}\")\n",
    "\n",
    "# Fine-tuning with grid search\n",
    "best = random_search.best_params_\n",
    "param_grid = {\n",
    "    \"n_estimators\": [best[\"n_estimators\"], best[\"n_estimators\"] + 150],\n",
    "    \"max_depth\": [max(3, best[\"max_depth\"]-2), best[\"max_depth\"], best[\"max_depth\"]+2],\n",
    "    \"min_samples_split\": [max(2, best[\"min_samples_split\"]-3), \n",
    "                          best[\"min_samples_split\"], \n",
    "                          best[\"min_samples_split\"]+3],\n",
    "    \"min_samples_leaf\": [max(1, best[\"min_samples_leaf\"]-1), \n",
    "                         best[\"min_samples_leaf\"], \n",
    "                         best[\"min_samples_leaf\"]+1],\n",
    "    \"max_features\": [max(0.1, best[\"max_features\"]-0.15), \n",
    "                     best[\"max_features\"], \n",
    "                     min(1.0, best[\"max_features\"]+0.15)],\n",
    "    \"bootstrap\": [best[\"bootstrap\"]],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    RandomForestClassifier(random_state=42, class_weight=\"balanced_subsample\", n_jobs=-1),\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"average_precision\",\n",
    "    cv=cv_strategy,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nPhase 2: Grid Search Refinement...\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(f\"Refined CV PR-AUC: {grid_search.best_score_:.4f}\")\n",
    "print(f\"Final parameters: {grid_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final model evaluation\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL MODEL PERFORMANCE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "final_model = grid_search.best_estimator_\n",
    "final_probs = final_model.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threshold Selection + Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding optimal threshold\n",
    "def find_optimal_threshold(y_true, probabilities):\n",
    "    \"\"\"find threshold that maximizes F1\"\"\"\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, probabilities)\n",
    "    \n",
    "    # Calculate F1 scores\n",
    "    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
    "    \n",
    "    # Find best threshold\n",
    "    best_idx = np.nanargmax(f1_scores)\n",
    "    optimal_threshold = thresholds[best_idx] if best_idx < len(thresholds) else 0.5\n",
    "    \n",
    "    return optimal_threshold, precision[best_idx], recall[best_idx], f1_scores[best_idx]\n",
    "\n",
    "# Find optimal threshold\n",
    "opt_thresh, opt_prec, opt_rec, opt_f1 = find_optimal_threshold(y_test, final_probs)\n",
    "print(f\"\\nOptimal threshold: {opt_thresh:.4f}\")\n",
    "print(f\"  Precision: {opt_prec:.3f}\")\n",
    "print(f\"  Recall: {opt_rec:.3f}\")\n",
    "print(f\"  F1-Score: {opt_f1:.3f}\")\n",
    "\n",
    "# Evaluate at optimal threshold\n",
    "final_preds = (final_probs >= opt_thresh).astype(int)\n",
    "final_cm = confusion_matrix(y_test, final_preds)\n",
    "\n",
    "print(f\"\\nFinal Test Performance:\")\n",
    "print(f\"  ROC-AUC: {roc_auc_score(y_test, final_probs):.4f}\")\n",
    "print(f\"  PR-AUC: {average_precision_score(y_test, final_probs):.4f}\")\n",
    "print(f\"\\nConfusion Matrix:\\n{final_cm}\")\n",
    "print(classification_report(y_test, final_preds, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export model\n",
    "import pickle\n",
    "\n",
    "model_package = {\n",
    "    \"classifier\": final_model,\n",
    "    \"optimal_threshold\": opt_thresh,\n",
    "    \"hyperparameters\": grid_search.best_params_,\n",
    "    \"cv_score\": grid_search.best_score_,\n",
    "    \"test_metrics\": {\n",
    "        \"roc_auc\": roc_auc_score(y_test, final_probs),\n",
    "        \"pr_auc\": average_precision_score(y_test, final_probs),\n",
    "        \"f1\": opt_f1\n",
    "    },\n",
    "    \"model_version\": \"1.0\",\n",
    "    \"notes\": \"Optimized Random Forest for lead qualification with custom threshold\"\n",
    "}\n",
    "\n",
    "with open(\"lead_scoring_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model_package, f)\n",
    "\n",
    "print(\"\\nModel saved to lead_scoring_model.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
