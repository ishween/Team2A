{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Sai\\Documents\\GitHub\\Team2A\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import re\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import chromadb\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import os\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ollama environment variables set for this notebook session.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# ollama model wasnt running, so lower memory usage\n",
        "os.environ[\"OLLAMA_GPU_OVERHEAD\"] = \"536870912\"\n",
        "os.environ[\"OLLAMA_FLASH_ATTENTION\"] = \"1\"\n",
        "os.environ[\"OLLAMA_KV_CACHE_QUANT\"] = \"1\"\n",
        "os.environ[\"OLLAMA_NUM_GPU\"] = \"10\"   \n",
        "os.environ[\"GGML_CUDA_ENABLE_UNIFIED_MEMORY\"] = \"1\"\n",
        "os.environ[\"OLLAMA_NUM_PARALLEL\"] = \"1\"\n",
        "os.environ[\"OLLAMA_CONTEXT_LENGTH\"] = \"2048\"\n",
        "os.environ[\"OLLAMA_NEW_ENGINE\"] = \"1\"\n",
        "\n",
        "print(\"Ollama environment variables set for this notebook session.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import ollama\n",
        "\n",
        "class GenAIAgent:\n",
        "    \"\"\"Base class for Gen AI agents that uses Ollama instead of HF models\"\"\"\n",
        "\n",
        "    def __init__(self, model_name=\"llama3\"):\n",
        "        self.model_name = model_name\n",
        "\n",
        "    def generate_response(self, prompt, temperature=0.7, max_new_tokens=10000):\n",
        "        \"\"\"Generate a response using Ollama\"\"\"\n",
        "        try:\n",
        "            response = ollama.generate(\n",
        "                model=self.model_name,\n",
        "                prompt=prompt,\n",
        "                options={\n",
        "                    \"temperature\": temperature,\n",
        "                    \"num_predict\": max_new_tokens\n",
        "                }\n",
        "            )\n",
        "\n",
        "            return response['response'].strip()\n",
        "        \n",
        "        except Exception as e:\n",
        "            return f\"Error generating response: {str(e)}\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# agent = GenAIAgent()\n",
        "# prompt = \"Hi how are you\"\n",
        "# response = agent.generate_response(prompt)\n",
        "# print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# FIXING INSIGHT GENERATION AGENT\n",
        "\n",
        "This prompt didn't require much changes; the prompt juust takes in the entire dataframe we have in our merged dataset and passes to the agent now"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "class InsightGenerationAgent(GenAIAgent):\n",
        "    \"\"\"Agent for generating business insights and recommendations\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # opportunities data is our MERGED dataframe\n",
        "    def generate_pipeline_insights(self, opportunities_data):\n",
        "        \"\"\"Generate intelligent pipeline insights\"\"\"\n",
        "\n",
        "        # Calculate pipeline metrics\n",
        "        total_pipeline = opportunities_data['close_value'].sum()\n",
        "        avg_deal_size = opportunities_data['close_value'].mean()\n",
        "        conversion_rate = len(opportunities_data[opportunities_data['deal_stage'] == 'Won']) / len(opportunities_data) * 100\n",
        "\n",
        "        context = f\"\"\"\n",
        "PIPELINE ANALYSIS DATA:\n",
        "- Total Pipeline Value: ${total_pipeline:,}\n",
        "- Number of Opportunities: {len(opportunities_data)}\n",
        "- Average Deal Size: ${avg_deal_size:,.0f}\n",
        "- Conversion Rate: {conversion_rate:.1f}%\n",
        "- Top Industries: {opportunities_data['sector'].value_counts().head(3).to_dict()}\n",
        "- Stage Distribution: {opportunities_data['deal_stage'].value_counts().to_dict()}\n",
        "- Top Performing Sales Agents: {opportunities_data['sales_agent'].value_counts().head(3).to_dict()}\n",
        "- Regional Breakdown: {opportunities_data['regional_office'].value_counts().to_dict()}\n",
        "\"\"\"\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "You are a senior sales analyst. Analyze the pipeline data and provide actionable insights:\n",
        "\n",
        "{context}\n",
        "\n",
        "Generate insights covering:\n",
        "1. Pipeline Health Assessment\n",
        "2. Deal Size and Conversion Trends\n",
        "3. Industry and Segment Performance\n",
        "4. Stage-Specific Recommendations\n",
        "5. Risk Factors and Mitigation Strategies\n",
        "6. Growth Opportunities\n",
        "\n",
        "Provide specific, actionable recommendations with data-driven reasoning.\n",
        "\n",
        "PIPELINE INSIGHTS:\"\"\"\n",
        "        insights = self.generate_response(prompt, temperature=0.2, max_new_tokens=350)\n",
        "        return insights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fixing ACCOUNT AGENT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "The account agent prompts all use the data we have from earlier BEFORE the merge (accounts, opportunity), however the columns/features that they expect from this data are different from what we have.\n",
        "\n",
        "For example, 'account_id' is needed, and these are also expected: \n",
        "'Account_name' \n",
        "'Industry' \n",
        "'Annual_revenue' \n",
        "'Employee_count' \n",
        "'Account_type' \n",
        "'region'\n",
        "'account_status'\n",
        "'account_owner'\n",
        "'created_date'\n",
        "'Last_activity'\n",
        "\n",
        "Some of those don't exist in our data, such as account_status, account_owner, so remove them from the prompts. Also added parent company into the prompt ('subsidiary_of') and added account_id in notebook 'data_fixing_for_agent' and have new csv in data/original called \"account_with_id.csv\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The opportunity agent similarly expects: \n",
        "'opportunity_name'\n",
        "'stage'\n",
        "'amount'\n",
        "'probability'\n",
        "'close_date'\n",
        "\n",
        "I subbed out ‘amount’ for ‘close_value’, calculated open_opps as open if the close date is not none, and changed ‘stage’ to ‘deal_stage’\n",
        "\n",
        "It expects an 'activities' dataframe which we don't have, but according to the code if we pass in nothing as the activity, it should be fine\n",
        "\n",
        "In summary, for the account agent, I fixed the prompts to pull the OUR columns from OUR data because it did not match our data before.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class AccountSummaryAgent(GenAIAgent):\n",
        "    \"\"\"Specialized agent for generating dynamic account summaries\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def create_summary_prompt(self, account_data, opportunities, activities, ml_insights=None):\n",
        "        \"\"\"Create dynamic prompt for account summary generation\"\"\"\n",
        "\n",
        "        # Calculate key metrics\n",
        "        total_pipeline = opportunities['close_value'].sum() if not opportunities.empty else 0\n",
        "        open_opps = len(opportunities[opportunities['close_date'].notna()])\n",
        "        won_opps = len(opportunities[opportunities['deal_stage'] == 'Won'])\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "You are an expert CRM analyst. Generate a comprehensive, professional account summary based on the following data:\n",
        "\n",
        "ACCOUNT INFORMATION:\n",
        "- Company: {account_data['account']}  \n",
        "- Industry: {account_data['sector']}\n",
        "- Annual Revenue: ${account_data['revenue']:,}\n",
        "- Employees: {account_data['employees']}\n",
        "- Region: {account_data['office_location']}\n",
        "- Parent company: {account_data['subsidiary_of'] if account_data['subsidiary_of'] else \"N/A\"}\n",
        "\n",
        "SALES PERFORMANCE:\n",
        "- Total Pipeline Value: ${total_pipeline:,}\n",
        "- Open Opportunities: {open_opps}\n",
        "- Won Opportunities: {won_opps}\n",
        "\n",
        "OPPORTUNITY DETAILS:\n",
        "{opportunities[['opportunity_id', 'sales_agent', 'product', 'deal_stage', 'engage_date', 'close_date', 'close_value']].to_string(index=False) if not opportunities.empty else \"No opportunities found\"}\n",
        "\n",
        "RECENT ACTIVITIES:\n",
        "{activities[['activity_type', 'subject', 'outcome', 'activity_date']].head(5).to_string(index=False) if not activities.empty else \"No recent activities\"}\n",
        "\n",
        "Generate a professional account summary that includes:\n",
        "1. Executive Overview (2-3 sentences about the account's status and potential)\n",
        "2. Key Metrics and Performance Indicators\n",
        "3. Opportunity Pipeline Analysis\n",
        "4. Engagement and Activity Summary\n",
        "5. Risk Assessment and Recommendations\n",
        "6. Next Steps and Action Items\n",
        "\n",
        "Format the response in clear sections with bullet points where appropriate. Be analytical, insights-driven, and actionable.\n",
        "\n",
        "ACCOUNT SUMMARY:\n",
        "        \"\"\"\n",
        "        return prompt\n",
        "\n",
        "    def generate_account_summary(self, account_data, opportunities, activities):\n",
        "        \"\"\"Generate dynamic account summary using LLM\"\"\"\n",
        "        prompt = self.create_summary_prompt(account_data, opportunities, activities)\n",
        "        summary = self.generate_response(prompt, temperature=0.3, max_new_tokens=400)\n",
        "\n",
        "        # Post-process to ensure quality\n",
        "        if len(summary) < 50:\n",
        "            return f\"\"\"\n",
        "**Account Summary: {account_data['account']}**\n",
        "\n",
        "**Executive Overview:**\n",
        "{account_data['account']} is a {account_data['sector'].lower()} company with ${account_data['revenue']:,} in annual revenue and {account_data['employees']} employees. This account is located the {account_data['office_location']} region and is a parent company of {account_data['subsidiary_of']}. \n",
        "\n",
        "**Key Metrics:**\n",
        "• Annual Revenue: ${account_data['revenue']:,}\n",
        "• Company Size: {account_data['employees']} employees\n",
        "\n",
        "\n",
        "**Pipeline Analysis:**\n",
        "• Total Opportunities: {len(opportunities)}\n",
        "• Pipeline Value: ${opportunities['close_value'].sum():,}\n",
        "• Open Deals: {len(opportunities[opportunities['close_date'].notna()])}\n",
        "\n",
        "\n",
        "**Recommendations:**\n",
        "Based on the account profile and activity level, focus on nurturing the relationship and identifying expansion opportunities.\n",
        "        \"\"\"\n",
        "\n",
        "        return summary\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fixing EMAIL DRAFT AGENT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "{\"The close date is approaching soon, so create urgency.\" if (opportunity_data['close_date'] - datetime.now()).days < 30 else \"\"}\n",
        "For this line, it calculates the current date difference from the close date. All the close dates are YEARRS ago so I delete this\n",
        "\n",
        "It expects opportunity['probability']. I think our model generates that so im gonna exclude it\n",
        "Theres also 'lead_data' which the introduction email uses, but we dont have it, i think can delete that for now\n",
        "I added additional details to the prompt that our dataset captures like the product series, engage date, etc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EmailDraftingAgent(GenAIAgent):\n",
        "    \"\"\"Specialized agent for generating contextual emails\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def create_email_prompt(self, email_type, context_data, additional_context=\"\"):\n",
        "        \"\"\"Create dynamic prompt for email generation\"\"\"\n",
        "\n",
        "        base_prompt = f\"\"\"\n",
        "You are an expert sales professional writing personalized, engaging emails. Generate a professional email based on the context provided.\n",
        "\n",
        "EMAIL TYPE: {email_type}\n",
        "CONTEXT DATA: {context_data}\n",
        "ADDITIONAL CONTEXT: {additional_context}\n",
        "\n",
        "Email Guidelines:\n",
        "- Professional but warm tone\n",
        "- Personalized and specific to the recipient\n",
        "- Clear call-to-action\n",
        "- Appropriate length (not too long or short)\n",
        "- Include relevant business value\n",
        "- Use compelling subject line\n",
        "- Follow best practices for sales communication\n",
        "\n",
        "Generate a complete email with:\n",
        "1. Subject Line\n",
        "2. Professional greeting\n",
        "3. Body with clear purpose and value proposition\n",
        "4. Specific call-to-action\n",
        "5. Professional closing\n",
        "\n",
        "EMAIL:\n",
        "        \"\"\"\n",
        "        return base_prompt\n",
        "\n",
        "    def draft_follow_up_email(self, opportunity_data, account_data, last_activity=None):\n",
        "        \"\"\"Draft intelligent follow-up email\"\"\"\n",
        "\n",
        "        context = f\"\"\"\n",
        "OPPORTUNITY: {opportunity_data['opportunity_id']}\n",
        "ACCOUNT: {account_data['account']}\n",
        "INDUSTRY: {account_data['sector']}\n",
        "CURRENT STAGE: {opportunity_data['deal_stage']}\n",
        "DEAL VALUE: ${opportunity_data['close_value']:,}\n",
        "CLOSE DATE: {opportunity_data['close_date']}\n",
        "LAST CONTACT: {last_activity if last_activity else 'No recent activity recorded'}\"\"\"\n",
        "\n",
        "        additional_context = f\"\"\"\n",
        "The opportunity is currently in {opportunity_data['deal_stage']} stage.\n",
        "Focus on moving the deal forward and addressing any potential concerns.\"\"\"\n",
        "\n",
        "        prompt = self.create_email_prompt(\"Follow-up\", context, additional_context)\n",
        "        email = self.generate_response(prompt, temperature=0.4, max_new_tokens=300)\n",
        "\n",
        "        return self.format_email_output(email, opportunity_data, account_data)\n",
        "\n",
        "\n",
        "# ignore??\n",
        "    # def draft_introduction_email(self, lead_data, additional_info=\"\"):\n",
        "    #     \"\"\"Draft introduction email for new leads\"\"\"\n",
        "\n",
        "    #     context = f\"\"\"\n",
        "    #     LEAD: {lead_data['first_name']} {lead_data['last_name']}\n",
        "    #     TITLE: {lead_data['title']}\n",
        "    #     COMPANY: {lead_data['company']}\n",
        "    #     INDUSTRY: {lead_data['sector']}\n",
        "    #     LEAD SCORE: {lead_data['lead_score']}/100\n",
        "    #     SOURCE: {lead_data['source']}\n",
        "    #     \"\"\"\n",
        "\n",
        "    #     additional_context = f\"\"\"\n",
        "    #     This is a new lead with a score of {lead_data['lead_score']}/100.\n",
        "    #     The lead came from {lead_data['source']}.\n",
        "    #     Focus on introducing your company's value proposition relevant to their industry ({lead_data['sector']}).\n",
        "    #     Keep it brief and focus on scheduling a discovery call.\n",
        "    #     \"\"\"\n",
        "\n",
        "    #     prompt = self.create_email_prompt(\"Introduction\", context, additional_context)\n",
        "    #     email = self.generate_response(prompt, temperature=0.5, max_new_tokens=250)\n",
        "\n",
        "    #     return self.format_lead_email_output(email, lead_data)\n",
        "\n",
        "    def draft_proposal_email(self, opportunity_data, account_data, proposal_details=\"\"):\n",
        "        \"\"\"Draft proposal presentation email\"\"\"\n",
        "        context = f\"\"\"\n",
        "OPPORTUNITY INFORMATION:\n",
        "- Opportunity ID: {opportunity_data['opportunity_id']}\n",
        "- Product: {opportunity_data['product']}\n",
        "- Deal Stage: {opportunity_data['deal_stage']}\n",
        "- Sales Agent: {opportunity_data['sales_agent']}\n",
        "- Manager: {opportunity_data['manager']}\n",
        "- Regional Office: {opportunity_data['regional_office']}\n",
        "- Series: {opportunity_data['series']}\n",
        "- Engage Date: {opportunity_data['engage_date']}\n",
        "- Expected Close Date: {opportunity_data['close_date']}\n",
        "- Deal Value: ${opportunity_data['close_value']:,}\n",
        "- Sales Price: ${opportunity_data['sales_price']:,}\n",
        "\n",
        "ACCOUNT INFORMATION:\n",
        "- Account Name: {opportunity_data['account']}\n",
        "- Industry (Sector): {opportunity_data['sector']}\n",
        "- Year Established: {opportunity_data['year_established']}\n",
        "- Annual Revenue: ${opportunity_data['revenue']:,}\n",
        "- Employees: {opportunity_data['employees']}\n",
        "- Office Location: {opportunity_data['office_location']}\n",
        "- Subsidiary Of: {opportunity_data['subsidiary_of']}\n",
        "\"\"\"\n",
        "\n",
        "        additional_context = f\"\"\"\n",
        "The opportunity is ready for proposal presentation.\n",
        "Focus on scheduling a meeting to present the proposal.\n",
        "Highlight the business value and ROI for their {account_data['sector']} industry.\n",
        "Create excitement about the solution and next steps.\n",
        "        \"\"\"\n",
        "\n",
        "        prompt = self.create_email_prompt(\"Proposal Presentation\", context, additional_context)\n",
        "        email = self.generate_response(prompt, temperature=0.3, max_new_tokens=280)\n",
        "\n",
        "        return self.format_email_output(email, opportunity_data, account_data)\n",
        "\n",
        "    def format_email_output(self, email_content, opportunity_data, account_data):\n",
        "        \"\"\"Format and enhance email output\"\"\"\n",
        "\n",
        "        # If the generated email is too short or incomplete, provide a structured fallback\n",
        "        if len(email_content.strip()) < 100:\n",
        "            return f\"\"\"\n",
        "Subject: Following up on {opportunity_data['opportunity_id']} - Next Steps\n",
        "\n",
        "Dear {account_data['account']} Team,\n",
        "\n",
        "I hope this email finds you well. I wanted to follow up on opportunity {opportunity_data['opportunity_id']} that we've been discussing.\n",
        "\n",
        "**Current Status:**\n",
        "• Deal Stage: {opportunity_data['deal_stage']}\n",
        "• Project Value: ${opportunity_data['close_value']:,}\n",
        "• Target Timeline: {opportunity_data['close_date']}\n",
        "\n",
        "**Next Steps:**\n",
        "I'd like to schedule a brief call this week to discuss any questions you might have and outline the next steps in our process. This will help ensure we stay on track for your {opportunity_data['close_date']} timeline.\n",
        "\n",
        "**Value Proposition:**\n",
        "Our solution is specifically designed for {account_data['sector']} companies like {account_data['account']}, helping organizations achieve measurable results while reducing operational complexity.\n",
        "\n",
        "Would you be available for a 30-minute call this week? I have openings on Tuesday and Thursday afternoons.\n",
        "\n",
        "Best regards,\n",
        "{opportunity_data['sales_agent']}\n",
        "\n",
        "P.S. I've attached some relevant case studies from similar {account_data['sector']} implementations that you might find interesting.\n",
        "\"\"\"\n",
        "\n",
        "        return email_content\n",
        "\n",
        "#     def format_lead_email_output(self, email_content, lead_data):\n",
        "#         \"\"\"Format lead introduction email output\"\"\"\n",
        "\n",
        "#         if len(email_content.strip()) < 80:\n",
        "#             return f\"\"\"\n",
        "# Subject: Introduction - Helping {lead_data['company']} Optimize {lead_data['sector']} Operations\n",
        "\n",
        "# Hello {lead_data['first_name']},\n",
        "\n",
        "# I hope this email finds you well. I'm reaching out because I noticed {lead_data['company']}'s recent activity and thought there might be an opportunity for us to help.\n",
        "\n",
        "# **Why I'm Reaching Out:**\n",
        "# We specialize in helping {lead_data['sector']} companies like yours streamline operations and drive growth. Given your role as {lead_data['title']}, I thought you'd be interested in learning how we've helped similar organizations achieve significant results.\n",
        "\n",
        "# **Quick Question:**\n",
        "# Are you currently facing any challenges with [specific industry challenge] that's impacting your team's efficiency or bottom line?\n",
        "\n",
        "# I'd love to share some quick insights that might be valuable for {lead_data['company']}. Would you be open to a brief 15-minute conversation this week?\n",
        "\n",
        "# Best regards,\n",
        "# {lead_data['owner']}\n",
        "\n",
        "# P.S. No sales pitch - just a quick exchange of ideas that might be mutually beneficial.\n",
        "# \"\"\"\n",
        "\n",
        "#         return email_content\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CRMChatbot:\n",
        "    def __init__(self, client, embedder, ml_models, accounts, opportunities, leads, activities):\n",
        "        self.client = client\n",
        "        self.embedder = embedder\n",
        "        self.ml_models = ml_models\n",
        "        self.accounts = accounts\n",
        "        self.opportunities = opportunities\n",
        "        self.leads = leads\n",
        "        self.activities = activities\n",
        "        self.llm = GenAIAgent()\n",
        "\n",
        "        # Initialize AI Agents\n",
        "        self.summary_agent = AccountSummaryAgent()\n",
        "        self.email_agent = EmailDraftingAgent()\n",
        "        self.insight_agent = InsightGenerationAgent()\n",
        "\n",
        "    def return_preprocessed_lead_scoring_data(self, df):\n",
        "        df[\"qualified_lead\"] = (\n",
        "            (df.get(\"deal_stage_ENGAGING\", 0) == 1) |\n",
        "            (df.get(\"deal_stage_WON\", 0) == 1) |\n",
        "            (df.get(\"deal_stage_LOST\", 0) == 1)\n",
        "        ).astype(int)\n",
        "        TARGET = \"qualified_lead\"\n",
        "        temporal_cols = ['engage_date', 'close_date', 'engage_year', 'engage_month', \n",
        "                        'engage_dayofweek', 'days_to_close', 'closed_within_30d']\n",
        "        outcome_cols = ['deal_stage_PROSPECTING', 'deal_stage_ENGAGING', \n",
        "                        'deal_stage_WON', 'deal_stage_LOST', 'won_deal', \n",
        "                        'has_close_date', 'close_value', 'close_value_log']\n",
        "        remove_cols = temporal_cols + outcome_cols\n",
        "        raw_features = ['revenue', 'employees', 'sales_price']\n",
        "        feature_cols = [c for c in df.columns if c not in remove_cols + raw_features + [TARGET]]\n",
        "        X = df[feature_cols]\n",
        "        return X.copy()\n",
        "\n",
        "    def generate_account_summary(self, account_id):\n",
        "        \"\"\"Generate dynamic account summary using AI agent\"\"\"\n",
        "        account = self.accounts[self.accounts['account_id'] == account_id]\n",
        "        if account.empty:\n",
        "            return f\"Account {account_id} not found.\"\n",
        "\n",
        "        account_data = account.iloc[0]\n",
        "        related_opps = self.opportunities[self.opportunities['account_id'] == account_id]\n",
        "        related_activities = self.activities[self.activities['account_id'] == account_id] if not self.activities.empty else pd.DataFrame()\n",
        "\n",
        "        # Use AI agent to generate dynamic summary\n",
        "        summary = self.summary_agent.generate_account_summary(account_data, related_opps, related_activities)\n",
        "        return summary\n",
        "\n",
        "    def draft_email(self, opportunity_id, email_type=\"follow_up\"):\n",
        "        \"\"\"Draft intelligent email using AI agent\"\"\"\n",
        "        opp = self.opportunities[self.opportunities['opportunity_id'] == opportunity_id]\n",
        "        if opp.empty:\n",
        "            return f\"Opportunity {opportunity_id} not found.\"\n",
        "\n",
        "        opp_data = opp.iloc[0]\n",
        "        account = self.accounts[self.accounts['account_id'] == opp_data['account_id']].iloc[0]\n",
        "\n",
        "        # Get last activity for context\n",
        "        last_activity = self.activities[\n",
        "            self.activities['account_id'] == opp_data['account_id']\n",
        "        ].sort_values('activity_date', ascending=False).iloc[0] if not self.activities.empty else None\n",
        "\n",
        "        # Use AI agent to draft email\n",
        "        if email_type == \"follow_up\":\n",
        "            email = self.email_agent.draft_follow_up_email(opp_data, account, last_activity)\n",
        "        elif email_type == \"proposal\":\n",
        "            email = self.email_agent.draft_proposal_email(opp_data, account)\n",
        "        else:\n",
        "            email = self.email_agent.draft_follow_up_email(opp_data, account, last_activity)\n",
        "\n",
        "        return email\n",
        "\n",
        "    # def draft_lead_email(self, lead_id):\n",
        "    #     \"\"\"Draft introduction email for leads\"\"\"\n",
        "    #     lead = self.leads[self.leads['lead_id'] == lead_id]\n",
        "    #     if lead.empty:\n",
        "    #         return f\"Lead {lead_id} not found.\"\n",
        "\n",
        "    #     lead_data = lead.iloc[0]\n",
        "    #     email = self.email_agent.draft_introduction_email(lead_data)\n",
        "    #     return email\n",
        "\n",
        "    def generate_insights(self, insight_type=\"pipeline\"):\n",
        "        \"\"\"Generate business insights using AI agent\"\"\"\n",
        "        if insight_type == \"pipeline\":\n",
        "            insights = self.insight_agent.generate_pipeline_insights(self.opportunities)\n",
        "            return insights\n",
        "        else:\n",
        "            return \"Insight type not supported yet.\"\n",
        "\n",
        "    def get_ml_insights(self, query, id):\n",
        "        \"\"\"Get ML model insights\"\"\"\n",
        "        insights = []\n",
        "        id = id.split(\"-\")[-1]  \n",
        "\n",
        "        query_lower = query.lower()\n",
        "\n",
        "        run_lead_scoring = ('lead' in query_lower or 'score' in query_lower)\n",
        "        run_account_health = ('account' in query_lower and 'health' in query_lower)\n",
        "        run_opp_win = ('opportunity' in query_lower and ('win' in query_lower or 'probability' in query_lower))\n",
        "\n",
        "        if run_lead_scoring:\n",
        "            # print(\"Running Lead Scoring Model\")\n",
        "            lead_scoring_df = pd.read_csv(os.path.join(os.getcwd(), \"..\", \"..\", \"data\", \"cleaned_data_with_id.csv\"))\n",
        "            lead_scoring_df = lead_scoring_df[lead_scoring_df['opportunity_id'] == id]\n",
        "            X = self.return_preprocessed_lead_scoring_data(lead_scoring_df)\n",
        "            X.drop(\"opportunity_id\", axis=1, inplace=True)\n",
        "            lead_scoring = self.ml_models['lead_scoring_model']\n",
        "            lead_scoring_result = lead_scoring.predict(X)[0]\n",
        "            insights.append(f\"Lead Scoring Prediction: {'High Quality Lead' if lead_scoring_result == 1 else 'Low Quality Lead'}\")\n",
        "\n",
        "        if run_account_health:\n",
        "            # print(\"Running Account Health Model\")\n",
        "            acc_health_df = pd.read_csv(os.path.join(os.getcwd(), \"..\", \"..\", \"data\", \"preprocessed_acc_health.csv\"))\n",
        "            X = acc_health_df[acc_health_df['opportunity_id'] == id].copy()\n",
        "            X.drop(\"opportunity_id\", axis=1, inplace=True)\n",
        "            acc_health_model = self.ml_models['account_health_model']\n",
        "            acc_health_result = acc_health_model.predict(X)[0]\n",
        "            insights.append(f\"Account Health Score Prediction: {acc_health_result}\")\n",
        "\n",
        "        if run_opp_win:\n",
        "            # print(\"Running Opportunity Win Model\")\n",
        "            opp_win_df = pd.read_csv(os.path.join(os.getcwd(), \"..\", \"..\", \"data\", \"preprocessed_opp_win_data.csv\"))\n",
        "            X = opp_win_df[opp_win_df['opportunity_id'] == id].copy()\n",
        "            X.drop('opportunity_id', axis=1, inplace=True)\n",
        "            opp_win_model = self.ml_models['opportunity_win_model']\n",
        "            opp_win_result = opp_win_model.predict(X)[0]\n",
        "            insights.append(f\"Opportunity Win Prediction: {'Win' if opp_win_result == 1 else 'Lose'}\")\n",
        "\n",
        "        if insights:\n",
        "            paragraph = \" \\n\".join(insights)\n",
        "\n",
        "            llm_prompt = f\"\"\"\n",
        "            Provide a clear, business-friendly summary of the following ML insights.\n",
        "            Focus on actionable points and avoid technical jargon.\n",
        "\n",
        "            === RAW INSIGHTS ===\n",
        "            {paragraph}\n",
        "            === END === \n",
        "\n",
        "            Your Summary:\n",
        "            \"\"\"\n",
        "\n",
        "            final_summary = self.llm.generate_response(llm_prompt)\n",
        "\n",
        "            return final_summary\n",
        "        else:\n",
        "            return \"No ML insights available for this query.\"\n",
        "\n",
        "    \n",
        "    def process_natural_language_query(self, query):\n",
        "        \"\"\"Process natural language queries and route to appropriate AI agents\"\"\"\n",
        "        query_lower = query.lower()\n",
        "\n",
        "        # Account summary requests\n",
        "        if any(word in query_lower for word in ['account', 'company']) and 'summary' in query_lower:\n",
        "            # Extract account ID or name from query\n",
        "            account_match = re.search(r'acc-\\d+', query_lower)\n",
        "            if account_match:\n",
        "                account_id = account_match.group().upper()\n",
        "                return self.generate_account_summary(account_id)\n",
        "            else:\n",
        "                return \"Please specify an account ID (e.g., ACC-00001) for the summary.\"\n",
        "\n",
        "        # Opportunity searches (this method was never implemented)\n",
        "        # elif 'opportunit' in query_lower and any(word in query_lower for word in ['find', 'show', 'list']):\n",
        "        #     criteria = query_lower\n",
        "        #     opps = self.find_opportunities_by_criteria(criteria)\n",
        "        #     return f\"Found {len(opps)} opportunities:\\n\\n\" + opps.to_string(index=False)\n",
        "\n",
        "        # Email drafting with AI agent\n",
        "        elif 'email' in query_lower or 'draft' in query_lower:\n",
        "            # Check for opportunity ID\n",
        "            opp_match = re.search(r'opp-[\\da-zA-Z]+', query_lower)\n",
        "            if opp_match:\n",
        "                opp_id = opp_match.group().upper()\n",
        "                email_type = \"proposal\" if 'proposal' in query_lower else \"follow_up\"\n",
        "                return self.draft_email(opp_id, email_type)\n",
        "            else:\n",
        "                return \"Please specify an opportunity ID (e.g., OPP-PEX0DEA) for email drafting.\"\n",
        "\n",
        "        # Business insights generation\n",
        "        elif any(word in query_lower for word in ['insights', 'analysis', 'pipeline', 'forecast']):\n",
        "            insight_type = \"pipeline\" if 'pipeline' in query_lower else \"general\"\n",
        "            return self.generate_insights(insight_type)\n",
        "\n",
        "        # ML insights\n",
        "        elif any(word in query_lower for word in ['predict', 'score', 'probability', 'ml']):\n",
        "            opp_match = re.search(r'opp-[\\da-zA-Z]+', query_lower)\n",
        "\n",
        "            if opp_match:\n",
        "                opp_id = opp_match.group().upper()\n",
        "            else:\n",
        "                return \"Please specify an opportunity ID (e.g., OPP-PEX0DEA) for ML insights.\"\n",
        "            \n",
        "            insights = self.get_ml_insights(query, id=opp_id)\n",
        "            return insights\n",
        "\n",
        "        # Semantic search fallback\n",
        "        else:\n",
        "            \"\"\"Semantic search fallback using ChromaDB + LLM contextual answer generation\"\"\"\n",
        "\n",
        "            acc_collection = self.client.get_collection(\"sales_accounts\")\n",
        "            opp_collection = self.client.get_collection(\"sales_opportunities\")\n",
        "\n",
        "            query_embedding = self.embedder.encode([query]).tolist()\n",
        "\n",
        "            acc_results = acc_collection.query(query_embeddings=query_embedding, n_results=3)\n",
        "            opp_results = opp_collection.query(query_embeddings=query_embedding, n_results=3)\n",
        "\n",
        "            context_blocks = []\n",
        "\n",
        "            if acc_results[\"documents\"] and acc_results[\"documents\"][0]:\n",
        "                account_context = []\n",
        "                for i, doc in enumerate(acc_results[\"documents\"][0][:3]):\n",
        "                    meta = acc_results[\"metadatas\"][0][i]\n",
        "                    account_context.append({\n",
        "                        \"document\": doc,\n",
        "                        \"metadata\": meta\n",
        "                    })\n",
        "                context_blocks.append({\"type\": \"accounts\", \"results\": account_context})\n",
        "\n",
        "            if opp_results[\"documents\"] and opp_results[\"documents\"][0]:\n",
        "                opp_context = []\n",
        "                for i, doc in enumerate(opp_results[\"documents\"][0][:3]):\n",
        "                    meta = opp_results[\"metadatas\"][0][i]\n",
        "                    opp_context.append({\n",
        "                        \"document\": doc,\n",
        "                        \"metadata\": meta\n",
        "                    })\n",
        "                context_blocks.append({\"type\": \"opportunities\", \"results\": opp_context})\n",
        "\n",
        "            if not context_blocks:\n",
        "                return self.llm.generate_response(\n",
        "                    f\"No direct matches found, but please answer this query thoughtfully:\\n\\n{query}\"\n",
        "                )\n",
        "\n",
        "            llm_prompt = f\"\"\"\n",
        "            You are an intelligent CRM assistant. Use the contextual search results \n",
        "            to answer the user's query with helpful, business-relevant guidance.\n",
        "\n",
        "            USER QUERY:\n",
        "            \"{query}\"\n",
        "\n",
        "            CONTEXT RESULTS (from semantic search):\n",
        "            {context_blocks}\n",
        "\n",
        "            TASK:\n",
        "            - Use the context to provide the best possible answer\n",
        "            - Highlight the most relevant accounts or opportunities when appropriate\n",
        "            - Be concise, professional, and helpful\n",
        "            - If context is weak, still try to give a meaningful answer\n",
        "\n",
        "            Final Answer:\n",
        "            \"\"\"\n",
        "\n",
        "            return self.llm.generate_response(llm_prompt)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "GenAIAgent's Role: The GenAIAgent (and its subclasses like AccountSummaryAgent) is designed specifically for Large Language Model (LLM) based text generation. Its __init__ method focuses on loading the LLM's tokenizer and model (microsoft/DialoGPT-medium in this case) into memory so it can generate human-like text responses.\n",
        "\n",
        "CRMVectorStore's Role: The CRMVectorStore class, which holds self.vector_store, has a completely different purpose: to manage and query vector embeddings for semantic search using SentenceTransformer and ChromaDB.\n",
        "\n",
        "CRMChatbot Orchestration: It's the CRMChatbot class that brings these different capabilities together. When you initialize chatbot = CRMChatbot(vector_store, ml_models, accounts_clean, ...), you are passing instances of CRMVectorStore (as vector_store), CRMMLModels (as ml_models), and also creating instances of AccountSummaryAgent, EmailDraftingAgent, and InsightGenerationAgent within the CRMChatbot.\n",
        "\n",
        "The CRMChatbot's process_natural_language_query method then decides which underlying component to call based on the user's query:\n",
        "\n",
        "If the query is for a summary, it calls self.summary_agent.generate_account_summary().\n",
        "\n",
        "If the query involves semantic search (e.g., 'Show me accounts in the technology industry'), it would use self.vector_store.semantic_search().\n",
        "\n",
        "So, while self.tokenizer and self.model are part of the text generation pipeline, they operate independently of the vector store, which handles retrieval."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Sai\\Documents\\GitHub\\Team2A\\venv\\Lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator DummyClassifier from version 1.7.2 when using version 1.5.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "c:\\Users\\Sai\\Documents\\GitHub\\Team2A\\venv\\Lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator DecisionTreeRegressor from version 1.7.2 when using version 1.5.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "c:\\Users\\Sai\\Documents\\GitHub\\Team2A\\venv\\Lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator GradientBoostingClassifier from version 1.7.2 when using version 1.5.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "chromadb.api.client.SharedSystemClient.clear_system_cache() # fixing chromadb  bug?\n",
        "\n",
        "client = chromadb.PersistentClient(path=os.path.join(os.getcwd(), \"..\", \"..\", \"data\", \"chroma_db\"))\n",
        "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "accounts = pd.read_csv(os.path.join(os.getcwd(), \"..\", \"..\", \"data\", \"accounts_with_id.csv\"))\n",
        "opp = pd.read_csv(os.path.join(os.getcwd(), \"..\", \"..\", \"data\", \"opp_data_with_id.csv\"))\n",
        "models_path = os.path.join(os.getcwd(), \"..\", \"..\", \"models\")\n",
        "\n",
        "import pickle\n",
        "\n",
        "opp_win = pickle.load(open(os.path.join(os.getcwd(), \"..\", \"..\", \"models\", \"opportunity_win_model.pkl\"), \"rb\"))\n",
        "lead_scoring = pickle.load(open(os.path.join(os.getcwd(), \"..\", \"..\", \"models\", \"lead_scoring_model.pkl\"), \"rb\"))['classifier']\n",
        "account_health = pickle.load(open(os.path.join(os.getcwd(), \"..\", \"..\", \"models\", \"account_health_model.pkl\"), \"rb\"))['model']\n",
        "\n",
        "ml_models = {\n",
        "    \"opportunity_win_model\": opp_win,\n",
        "    \"lead_scoring_model\": lead_scoring,\n",
        "    \"account_health_model\": account_health\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "chatbot = CRMChatbot(client, embedder, ml_models, accounts, opp, leads=pd.DataFrame(), activities=pd.DataFrame())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Sai\\Documents\\GitHub\\Team2A\\venv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but ElasticNetCV was fitted without feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Based on the raw insights, here's a clear and actionable summary:\n",
            "\n",
            "**Insight:** Our lead scoring model predicts that high-quality leads are likely to be generated.\n",
            "\n",
            "**Key Point:** The Account Health Score prediction is 64.78001591196772, indicating that our current approach is effective in identifying high-value leads (above 50% score).\n",
            "\n",
            "**Actionable Takeaways:**\n",
            "\n",
            "1. **Confirm Lead Quality**: Continue to prioritize and nurture high-scoring leads, as they are likely to convert into customers.\n",
            "2. **Refine the Model**: Use this insight to refine your lead scoring model by adjusting weights or incorporating additional factors to improve its accuracy.\n",
            "3. **Streamline Follow-up**: Focus on targeted follow-ups with these high-quality leads to maximize conversion rates and reduce waste.\n",
            "4. **Monitor and Adjust**: Regularly review Account Health Scores to ensure the model remains effective in identifying high-value leads and adjust as needed.\n",
            "\n",
            "By leveraging this insight, you can optimize your lead generation strategy, improve conversion rates, and drive business growth.\n"
          ]
        }
      ],
      "source": [
        "# ml insights\n",
        "query = \"give me ML predictions for lead scoring, oppportunity win, account health with OPP-Z063OYW0\"\n",
        "response = chatbot.process_natural_language_query(query)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**Pipeline Health Assessment**\n",
            "\n",
            "The total pipeline value of $10,005,534 is a strong indicator of the company's sales performance. With 8800 opportunities in the pipeline, there is a good mix of potential deals to pursue. The conversion rate of 48.2% suggests that the sales team is doing a decent job of converting leads into customers.\n",
            "\n",
            "**Deal Size and Conversion Trends**\n",
            "\n",
            "The average deal size of $1,491 indicates that the company is targeting mid-sized deals. However, it's essential to note that the conversion rate varies across different deal sizes. Deals above $5,000 have a higher conversion rate (52.3%) compared to those below $2,000 (44.6%). This suggests that the sales team may be more effective at closing larger deals.\n",
            "\n",
            "Recommendation: Focus on upselling and cross-selling strategies to target larger deals, which can lead to increased revenue and profitability.\n",
            "\n",
            "**Industry and Segment Performance**\n",
            "\n",
            "The top industries in the pipeline are retail, technology, and medical, accounting for 54.4% of the total opportunities. This highlights the importance of these sectors in driving sales growth.\n",
            "\n",
            "Recommendation: Develop targeted marketing campaigns and sales strategies tailored to these industries to capitalize on their potential.\n",
            "\n",
            "**Stage-Specific Recommendations**\n",
            "\n",
            "* **Won**: With 4238 deals won, it's essential to focus on nurturing these relationships to ensure high customer satisfaction and loyalty.\n",
            "\t+ Recommendation: Implement a customer success program to ensure timely and effective support for these customers.\n",
            "* **Lost**: Analyzing the lost opportunities can help identify areas for improvement. For instance, 2473 deals were lost in the prospecting stage, indicating that the sales team may need to refine their lead generation strategies.\n",
            "\t+ Recommendation:\n"
          ]
        }
      ],
      "source": [
        "# insight agent test\n",
        "query = \"give me pipeline insights\"\n",
        "response = chatbot.process_natural_language_query(query)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# email agent test\u001b[39;00m\n\u001b[32m      2\u001b[39m query = \u001b[33m\"\u001b[39m\u001b[33mgive me proposal email for OPP-Z063OYW0\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m response = \u001b[43mchatbot\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprocess_natural_language_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(response)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 178\u001b[39m, in \u001b[36mCRMChatbot.process_natural_language_query\u001b[39m\u001b[34m(self, query)\u001b[39m\n\u001b[32m    176\u001b[39m     opp_id = opp_match.group().upper()\n\u001b[32m    177\u001b[39m     email_type = \u001b[33m\"\u001b[39m\u001b[33mproposal\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mproposal\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m query_lower \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mfollow_up\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdraft_email\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopp_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43memail_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mPlease specify an opportunity ID (e.g., OPP-PEX0DEA) for email drafting.\u001b[39m\u001b[33m\"\u001b[39m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 67\u001b[39m, in \u001b[36mCRMChatbot.draft_email\u001b[39m\u001b[34m(self, opportunity_id, email_type)\u001b[39m\n\u001b[32m     65\u001b[39m     email = \u001b[38;5;28mself\u001b[39m.email_agent.draft_follow_up_email(opp_data, account, last_activity)\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m email_type == \u001b[33m\"\u001b[39m\u001b[33mproposal\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m     email = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43memail_agent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdraft_proposal_email\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopp_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccount\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     69\u001b[39m     email = \u001b[38;5;28mself\u001b[39m.email_agent.draft_follow_up_email(opp_data, account, last_activity)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 118\u001b[39m, in \u001b[36mEmailDraftingAgent.draft_proposal_email\u001b[39m\u001b[34m(self, opportunity_data, account_data, proposal_details)\u001b[39m\n\u001b[32m    110\u001b[39m         additional_context = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[33mThe opportunity is ready for proposal presentation.\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[33mFocus on scheduling a meeting to present the proposal.\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[33mHighlight the business value and ROI for their \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccount_data[\u001b[33m'\u001b[39m\u001b[33msector\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m industry.\u001b[39m\n\u001b[32m    114\u001b[39m \u001b[33mCreate excitement about the solution and next steps.\u001b[39m\n\u001b[32m    115\u001b[39m \u001b[33m        \u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m    117\u001b[39m         prompt = \u001b[38;5;28mself\u001b[39m.create_email_prompt(\u001b[33m\"\u001b[39m\u001b[33mProposal Presentation\u001b[39m\u001b[33m\"\u001b[39m, context, additional_context)\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m         email = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m280\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.format_email_output(email, opportunity_data, account_data)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mGenAIAgent.generate_response\u001b[39m\u001b[34m(self, prompt, temperature, max_new_tokens)\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Generate a response using Ollama\"\"\"\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     response = \u001b[43mollama\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnum_predict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response[\u001b[33m'\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m'\u001b[39m].strip()\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Sai\\Documents\\GitHub\\Team2A\\venv\\Lib\\site-packages\\ollama\\_client.py:256\u001b[39m, in \u001b[36mClient.generate\u001b[39m\u001b[34m(self, model, prompt, suffix, system, template, context, stream, think, raw, format, images, options, keep_alive)\u001b[39m\n\u001b[32m    229\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate\u001b[39m(\n\u001b[32m    230\u001b[39m   \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    231\u001b[39m   model: \u001b[38;5;28mstr\u001b[39m = \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    244\u001b[39m   keep_alive: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    245\u001b[39m ) -> Union[GenerateResponse, Iterator[GenerateResponse]]:\n\u001b[32m    246\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    247\u001b[39m \u001b[33;03m  Create a response using the requested model.\u001b[39;00m\n\u001b[32m    248\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    253\u001b[39m \u001b[33;03m  Returns `GenerateResponse` if `stream` is `False`, otherwise returns a `GenerateResponse` generator.\u001b[39;00m\n\u001b[32m    254\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m    \u001b[49m\u001b[43mGenerateResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mPOST\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/api/generate\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mGenerateRequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m      \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m      \u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m=\u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m      \u001b[49m\u001b[43msystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43msystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m      \u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m      \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m      \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m      \u001b[49m\u001b[43mthink\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthink\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m      \u001b[49m\u001b[43mraw\u001b[49m\u001b[43m=\u001b[49m\u001b[43mraw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[43m      \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m      \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_copy_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m      \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m      \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_dump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexclude_none\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    276\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Sai\\Documents\\GitHub\\Team2A\\venv\\Lib\\site-packages\\ollama\\_client.py:189\u001b[39m, in \u001b[36mClient._request\u001b[39m\u001b[34m(self, cls, stream, *args, **kwargs)\u001b[39m\n\u001b[32m    185\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(**part)\n\u001b[32m    187\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(**\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m.json())\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Sai\\Documents\\GitHub\\Team2A\\venv\\Lib\\site-packages\\ollama\\_client.py:129\u001b[39m, in \u001b[36mClient._request_raw\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_request_raw\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m    128\u001b[39m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m     r = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    130\u001b[39m     r.raise_for_status()\n\u001b[32m    131\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Sai\\Documents\\GitHub\\Team2A\\venv\\Lib\\site-packages\\httpx\\_client.py:825\u001b[39m, in \u001b[36mClient.request\u001b[39m\u001b[34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[39m\n\u001b[32m    810\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m    812\u001b[39m request = \u001b[38;5;28mself\u001b[39m.build_request(\n\u001b[32m    813\u001b[39m     method=method,\n\u001b[32m    814\u001b[39m     url=url,\n\u001b[32m   (...)\u001b[39m\u001b[32m    823\u001b[39m     extensions=extensions,\n\u001b[32m    824\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Sai\\Documents\\GitHub\\Team2A\\venv\\Lib\\site-packages\\httpx\\_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Sai\\Documents\\GitHub\\Team2A\\venv\\Lib\\site-packages\\httpx\\_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    939\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    948\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Sai\\Documents\\GitHub\\Team2A\\venv\\Lib\\site-packages\\httpx\\_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    981\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Sai\\Documents\\GitHub\\Team2A\\venv\\Lib\\site-packages\\httpx\\_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1010\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1011\u001b[39m     )\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n\u001b[32m   1018\u001b[39m response.request = request\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Sai\\Documents\\GitHub\\Team2A\\venv\\Lib\\site-packages\\httpx\\_transports\\default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    255\u001b[39m     status_code=resp.status,\n\u001b[32m    256\u001b[39m     headers=resp.headers,\n\u001b[32m    257\u001b[39m     stream=ResponseStream(resp.stream),\n\u001b[32m    258\u001b[39m     extensions=resp.extensions,\n\u001b[32m    259\u001b[39m )\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Sai\\Documents\\GitHub\\Team2A\\venv\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.Iterable)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Sai\\Documents\\GitHub\\Team2A\\venv\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Sai\\Documents\\GitHub\\Team2A\\venv\\Lib\\site-packages\\httpcore\\_sync\\connection.py:103\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Sai\\Documents\\GitHub\\Team2A\\venv\\Lib\\site-packages\\httpcore\\_sync\\http11.py:136\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mresponse_closed\u001b[39m\u001b[33m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    135\u001b[39m         \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Sai\\Documents\\GitHub\\Team2A\\venv\\Lib\\site-packages\\httpcore\\_sync\\http11.py:106\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n\u001b[32m    114\u001b[39m network_stream = \u001b[38;5;28mself\u001b[39m._network_stream\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Sai\\Documents\\GitHub\\Team2A\\venv\\Lib\\site-packages\\httpcore\\_sync\\http11.py:177\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    174\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Sai\\Documents\\GitHub\\Team2A\\venv\\Lib\\site-packages\\httpcore\\_sync\\http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Sai\\Documents\\GitHub\\Team2A\\venv\\Lib\\site-packages\\httpcore\\_backends\\sync.py:128\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "# email agent test\n",
        "query = \"give me proposal email for OPP-Z063OYW0\"\n",
        "response = chatbot.process_natural_language_query(query)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ml insights\n",
        "query = \"give me pipeline insights\"\n",
        "response = chatbot.process_natural_language_query(query)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# account summary agent test\n",
        "query = \"give me account summary for ACC-00007\"\n",
        "response = chatbot.process_natural_language_query(query)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Gradio Interface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7860\n",
            "* Running on public URL: https://5d2aef86e3c445045e.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://5d2aef86e3c445045e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Sai\\Documents\\GitHub\\Team2A\\venv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but ElasticNetCV was fitted without feature names\n",
            "  warnings.warn(\n",
            "c:\\Users\\Sai\\Documents\\GitHub\\Team2A\\venv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but ElasticNetCV was fitted without feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import gradio as gr\n",
        "\n",
        "def chat_with_crm(user_message, history):\n",
        "    \"\"\"\n",
        "    Gradio callback: takes user input and returns chatbot response.\n",
        "    `history` is the previous turns, but we only need `user_message`\n",
        "    because CRMChatbot is stateless per query.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = chatbot.process_natural_language_query(user_message)\n",
        "    except Exception as e:\n",
        "        response = f\"Error while processing query: {e}\"\n",
        "\n",
        "    return response\n",
        "\n",
        "demo = gr.ChatInterface(\n",
        "    fn=chat_with_crm,\n",
        "    title=\"Team 2A CRM Assistant\",\n",
        "    description=\"Ask questions about accounts, opportunities, pipeline insights, or draft emails.\",\n",
        ")\n",
        "\n",
        "demo.launch(share=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
